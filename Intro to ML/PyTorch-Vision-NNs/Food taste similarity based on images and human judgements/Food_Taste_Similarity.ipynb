{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a1314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2562b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61615b",
   "metadata": {},
   "source": [
    "### Tensor transformation of training dataset and Embeddings generation function with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d5b614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings():\n",
    "    \"\"\"\n",
    "    Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "    the embeddings.\n",
    "    \"\"\"\n",
    "    # TODO: define a transform to pre-process the images\n",
    "    \n",
    "    train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                           transforms.Resize(256),\n",
    "                                           transforms.CenterCrop(224),\n",
    "                                           transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms) \n",
    "    \n",
    "    # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n",
    "    # run out of memory\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              pin_memory=True, num_workers=8)\n",
    "\n",
    "    # TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n",
    "    #  more info here: https://pytorch.org/vision/stable/models.html)\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    embeddings = []\n",
    "    embedding_size = model.fc.in_features \n",
    "    num_images = len(train_dataset)\n",
    "    embeddings = np.zeros((num_images, embedding_size))\n",
    "    # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n",
    "    # model to access the embeddings the model generates. \n",
    "\n",
    "    # Removal of last layer of pretrained model\n",
    "    newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "    print(newmodel)\n",
    "\n",
    "    #embeddings creation \n",
    "    counter = 0\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        print(counter)\n",
    "        counter = counter + 1\n",
    "        with torch.no_grad():\n",
    "            newmodel.eval()\n",
    "            tmp1 = newmodel.forward(data)\n",
    "            tmp2 = torch.reshape(tmp1,(2048,))\n",
    "            embeddings[idx,:] = tmp2.numpy()\n",
    "\n",
    "    np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9900a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9482fb32",
   "metadata": {},
   "source": [
    "### Training Data processing (Tripplets, Embeddings Normalization, Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b108da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, train=True):\n",
    "    \"\"\"\n",
    "    Load the triplets from the file and generate the features and labels.\n",
    "\n",
    "    input: file: string, the path to the file containing the triplets\n",
    "          train: boolean, whether the data is for training or testing\n",
    "\n",
    "    output: X: numpy array, the features\n",
    "            y: numpy array, the labels\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "\n",
    "    # generate training data from triplets\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n",
    "                                         transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n",
    "    embeddings = np.load('dataset/embeddings.npy')\n",
    "    \n",
    "    # TODO: Normalize the embeddings across the dataset\n",
    " \n",
    "    shape = np.shape(embeddings)\n",
    "    \n",
    "    for i in range(shape[1]):\n",
    "        mean = np.mean(embeddings[:,i])\n",
    "        std = np.std(embeddings[:,i])\n",
    "        embeddings[:,i] = (embeddings[:,i]- mean)/std\n",
    "    \n",
    "\n",
    "    file_to_embedding = {}\n",
    "    for i in range(len(filenames)):\n",
    "        file_to_embedding[filenames[i]] = embeddings[i]\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # use the individual embeddings to generate the features and labels for triplets\n",
    "    for t in triplets:\n",
    "        emb = [file_to_embedding[a] for a in t.split()]\n",
    "        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n",
    "        y.append(1)\n",
    "        # Generating negative samples (data augmentation)\n",
    "        if train:\n",
    "            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n",
    "            y.append(0)\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ae9c5",
   "metadata": {},
   "source": [
    "### Training and Test Data definition and tensor creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bcaa74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory\n",
    "def create_loader_from_np(X, y = None, train = True, batch_size=64, shuffle=True, num_workers = 8):\n",
    "    \"\"\"\n",
    "    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n",
    "\n",
    "    input: X: numpy array, the features\n",
    "           y: numpy array, the labels\n",
    "    \n",
    "    output: loader: torch.data.util.DataLoader, the object containing the data\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n",
    "                                torch.from_numpy(y).type(torch.long))\n",
    "                \n",
    "        # Define train/test split\n",
    "        train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "        # Define samplers for train and test data\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        val_sampler = SubsetRandomSampler(val_indices)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=len(val_indices), sampler=val_sampler)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "        \n",
    "    else:\n",
    "        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n",
    "        loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle,\n",
    "                        pin_memory=True, num_workers=num_workers)\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ebd74",
   "metadata": {},
   "source": [
    "### Definition of Neural Network model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aba0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define network architecture\n",
    "        self.fc1 = nn.Linear(6144, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(p=0.7)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ae318",
   "metadata": {},
   "source": [
    "### Model Training function, performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f23fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    The training procedure of the model; it accepts the training data, defines the model \n",
    "    and then trains it.\n",
    "\n",
    "    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n",
    "    \n",
    "    output: model: torch.nn.Module, the trained model\n",
    "    \"\"\"\n",
    "    model = Net() \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    n_epochs = 10\n",
    "    \n",
    "    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n",
    "    # of the training data as a validation split. After each epoch, compute the loss on the \n",
    "    # validation split and print it out. This enables you to see how your model is performing \n",
    "    # on the validation data before submitting the results on the server. After choosing the \n",
    "    # best model, train it on the whole training data.\n",
    "    \n",
    "    #loss and optimizer from tutorial\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    losses = []\n",
    "    loss_mean = []\n",
    "    \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for [X,y] in train_loader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            y_pred = model.forward(X)\n",
    "            y_pred_rounded = torch.round(y_pred)\n",
    "            \n",
    "            #make loss work\n",
    "            y = y.view(y.size()[0], 1)\n",
    "            y = y.float()\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        loss_mean.append(np.mean(losses)) \n",
    "    \n",
    "    # Plot the loss\n",
    "    if not os.path.exists('loss_results'):\n",
    "        os.makedirs('loss_results')\n",
    "        \n",
    "    axes = plt.plot([i for i in range(n_epochs)], loss_mean)\n",
    "    plt.savefig(\"loss_results/loss_plot.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for [X, y] in val_loader:\n",
    "            y_pred = model.forward(X)\n",
    "            \n",
    "            y = y.view(y.size()[0], 1)\n",
    "            y = y.float()\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "        val_loss = np.mean(losses)\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde6997",
   "metadata": {},
   "source": [
    "### Testing fo model to test data and predictions/results extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b8677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "    \"\"\"\n",
    "    The testing procedure of the model; it accepts the testing data and the trained model and \n",
    "    then tests the model on it.\n",
    "\n",
    "    input: model: torch.nn.Module, the trained model\n",
    "           loader: torch.data.util.DataLoader, the object containing the testing data\n",
    "        \n",
    "    output: None, the function saves the predictions to a results.txt file\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "        for [x_batch] in loader:\n",
    "            x_batch= x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            # Rounding the predictions to 0 or 1\n",
    "            predicted[predicted >= 0.5] = 1\n",
    "            predicted[predicted < 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "        predictions = np.vstack(predictions)\n",
    "    np.savetxt(\"results.txt\", predictions, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "362d7f95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x6144 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# define a model and train it\u001b[39;00m\n\u001b[1;32m     22\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msay \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart training\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msay \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining done\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# test the model on the test data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader)\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m y_pred_rounded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(y_pred)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#make loss work\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 27\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Forward pass through the network\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)))\n",
      "File \u001b[0;32m~/Python/SoccerAnalytics/SAenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/SoccerAnalytics/SAenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Python/SoccerAnalytics/SAenv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x6144 and 512x256)"
     ]
    }
   ],
   "source": [
    "# Main function. You don't have to change this\n",
    "if __name__ == '__main__':\n",
    "    TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "    TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "    # generate embedding for each image in the dataset\n",
    "    os.system('say \"embeddings generation start\"')\n",
    "    if(os.path.exists('dataset/embeddings.npy') == False):\n",
    "        generate_embeddings()\n",
    "        \n",
    "    os.system('say \"embeddings generation done\"')\n",
    "\n",
    "    # load the training and testing data\n",
    "    X, y = get_data(TRAIN_TRIPLETS)\n",
    "    X_test, _ = get_data(TEST_TRIPLETS, train=False)\n",
    "\n",
    "    # Create data loaders for the training and testing data\n",
    "    train_loader, val_loader = create_loader_from_np(X, y, train = True, batch_size=64)\n",
    "    test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n",
    "\n",
    "    # define a model and train it\n",
    "    os.system('say \"start training\"')\n",
    "    model = train_model(train_loader,val_loader)\n",
    "    os.system('say \"training done\"')\n",
    "    \n",
    "    # test the model on the test data\n",
    "    os.system('say \"start testing\"')\n",
    "    test_model(model, test_loader)\n",
    "    os.system('say \"testing done\"')\n",
    "    \n",
    "    #save stats\n",
    "    \n",
    "    currentmodel = Net()\n",
    "    info_str = str(currentmodel)\n",
    "    with open('loss_results/net_info.txt', 'w') as f:\n",
    "        f.write(info_str)\n",
    "    \n",
    "    \n",
    "    print(\"Results saved to results.txt\")\n",
    "    os.system('say \"finished\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
